---
title: "Introduction to Artificial Neural Networks"
date: 2021-10-1
categories: [AI and ML]
tags: [Machine-Learning, ANN]
comments: true
---

## Introduction

This guide will serve as an introduction to ANNs. We will build a neural network from scratch without using any specialised libraries like keras or pytorch. We will only use NumPy for matrix operations and other maths functions.

The best way to learn about ANN is to solve a real world problem. We will try to solve a classification problem using a neural network.

## The Problem

We will be tackling the problem of digit classification. We will be given a grayscale image of a handwritten digit (between 0 and 9) and our goal is to correctly identify it. This is a beginner level problem and is ideal for understanding the workings of a neural net.

The dataset that we will use is known as MNIST dataset. It is provided on Kaggle. Use the link below to download it.
[MNIST dataset](https://www.kaggle.com/datasets/hojjatk/mnist-dataset)

## About the Dataset

The dataset consists of thousands of images of handwritten digits. Each image is grayscale, and it has fixed dimensions of 28x28 pixels. So all in all, each image is 784 pixels of values ranging from 0 to 255. Since all the images are grayscale, we need not worry about pixel colours. Each pixel can only have values between 0 and 255.

## Matrix Representations

Lets assume we have **m** images.
If we represent our dataset as a matrix, then we have a matrix of dimensions **m x 784**, where rows are representing the number of images and columns are representing the number of pixels in each image. Let's denote this matrix by **X**.
<img src="/assets/images/ANN/input-matrix.png" alt="Alt text" />

We will now take transpose of this matrix. We do this so that future calculations are made easy for us. This is a completely optional step and it's only purpose is ease of calculations.
So, now we have a matrix of dimensions **784 x m**, where each image is now represented in a column instead of a row.
<img src="/assets/images/ANN/matrix-transpose.png" alt="Alt text" />

## Architecture

We will build a very simple neural network that will only have 3 layers in total. The 0th layer is called the input layer, and the last layer is called the output layer. Since, input layer neurons do not have any calculations associated with them, we say that our model only has 2 computational layers, i.e., 1 hidden and 1 output.

The input layer will consist of 784 neurons, representing 784 pixels in an image. The hidden layer will have 10 neurons, and the output layer will also have 10 neurons, representing the digits 0 to 9. And so, our model diagram will look something like this :
<img src="/assets/images/ANN/layers.png" alt="Alt text" />

## Notations

- We will denote the input layer using A<sup>[0]</sup>.
  It is just the input matrix of dimensions `(784 x m)`.
- The hidden layer will be denoted by Z<sup>[1]</sup>.
<br>
It is a matrix of dimensions `(10 x m)` and its value is calculated using,
<div align="center">
Z<sup>[1]</sup> = W<sup>[1]</sup> . A<sup>[0]</sup> + b<sup>[1]</sup>
</div>
<br>
Where, W<sup>[1]</sup> represents weight matrix(10 x 784) and b<sup>[1]</sup> represents bias matrix(10 x 1) of the first hidden layer.

## Training an ANN

There are 3 parts to training a neural net.

1. Feed forward : Take an input image > run through the network > compute the output.
2. Activation functions : Introduction of non linearity in our neural net.
3. Back propagation : Feedbacks from the difference between network's output and label.

## Building the neural net

### Activation function

We introduce a non-linearity in our hidden layer. Otherwise our neural net will only calculate a linear combination of values and behave like linear regression regardless of the number of hidden layers. There are a number of activation functions available, like, `sigmoid`, `tan h`, `ReLU`, etc. In our model, we will use `ReLU(x)`. It stands for **Rectified Linear Unit** and is defined as:

```text
ReLU(x) = x, x > 0
          0, x ≤ 0
```

<img src="/assets/images/ANN/relu-graph.jpeg" alt="Alt text" />

Every layer after the input layer is going to have activation function applied to it. Hence, generalizing the formula for every layer , we get,

<div align="center">
A<sup>[x]</sup> = ReLU(Z<sup>[x]</sup>)
</div>
<br>
Substituting this equation in the previous one, and generalizing it, we get,
<div align="center">
Z<sup>[n]</sup> = W<sup>[n]</sup> . ReLU(Z<sup>[n - 1]</sup>) + b<sup>[n]</sup> , ∀ n ∈ m and n ≠ 0
</div>
<br>
Now, for the second layer, which in our case is the output layer, we are going to apply a second activation function called `softmax`.

<img src="/assets/images/ANN/softmax.webp" alt="Alt text" />

`softmax` function converts the raw output of a neural net into a vector of probabilities with values lying between 0 and 1. This essentially provides us with a probability distribution over the input classes. In our case, we have 10 input classes. They are the digits 0 to 9. So, for a 784 pixel input image, we get a probability of that image being one of the 10 digits. Hence, our neural net is succesfully created.

### Back propagation

Adjust the weights and biases of the layers based on the feedback received from the output. <br>
We get the prediction from our neural net and then matching it to our labeled dataset, we get the deviation/error. Then, we see how much of each of the weights and biases contributed to this error and finally, we adjust the weights and biases.

In back propagation, we start from the end of the network. Hence, we first calculate the error in the last layer. This is also known as our `loss function`.

<div align="center">
dZ<sup>[2]</sup> = A<sup>[2]</sup> - Y
</div>
<br>
A<sup>[2]</sup> : output of the second layer<br>
Y : one-hot encoded labels
<br>
<br>
**One-hot encoding** : it is a method for converting categorical variables into a binary format. Let's say, we get the image of 5 as our input. Our inputs would be 784 pixels and the label will be 5. However, we are not going to directly subtract 5 from our network's output. Instead we perform one-hot encoding and `Y` for this example becomes a vector with all other labels set to 0, and 5 set to 1,

```text
Y = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] # encoded label
                    ↑
                    5  # actual label
```

Now that we have our encoded labels, we can get to finding out how much our weights(W) and biases(b) contributed to the output.
<br>

**<u>Second layer weights contribution</u>**

We take the derivative of the loss function with respect to weights.

dW<sup>[2]</sup>

To be continued...
